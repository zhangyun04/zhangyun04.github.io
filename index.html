<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Zhangyun's Homepage</title>
  
  <meta name="author" content="Zhangyun Tan">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
	<link rel="icon" href="images/ICON.jpg">
</head>
 
<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr>
      <td halign="center">
          <p align="center">
              <font size="6">Zhangyun Tan </font>
          </p>
      </td>
    </tr>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <!-- <p style="text-align:center">
                <name>Chao Huang</name>
              </p> -->
              <p>
                I am a undergrad student in the Department of Computer Science at the University of Rochester, advised by <a href="https://www.cs.rochester.edu/~cxu22/">Prof. Chenliang Xu</a>.  I received my B.Eng. from CS Department, Huazhong University of Science and Technology in 2022. In my undergrad, I worked with <a href="https://scholar.google.com/citations?user=YTQnGJsAAAAJ">Prof. He</a> and <a href="https://xiaosenwang.com/">Mr. Wang</a> at HUST on adversarial machine learning. I also work closely with <a href="https://scholar.google.com/citations?user=J9FPMToAAAAJ">Prof. Peng</a> at PKU on gradient estimation (Zeroth-Order optimization), and <a href="https://scholar.google.com/citations?user=C83b8ncAAAAJ&hl=en&oi=ao">Prof. Liu</a> at RPI/Columbia on high-performance quantum and tensor computation. I am good at playing Erhu and familiar with Violin. Welcome to reach out to chat:)
                <br>
                <br>
                Currently, I mainly work on efficient and reliable AI for my Ph.D. degree. 
                <br> 
              </p>
              <p style="text-align:center">
                <a href="mailto:zzh136@ur.rochester.edu">Email</a> &nbsp/&nbsp
                <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp/&nbsp -->
                <a href="https://scholar.google.com/citations?user=7nLfsSgAAAAJ&hl=en">Google Scholar</a>  &nbsp/&nbsp
                <!-- <a href="https://twitter.com/jon_barron">Twitter</a> &nbsp/&nbsp -->
                <a href="https://github.com/ZhangAIPI">Github</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/me.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/me.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
          <td style="padding:20px;width:100%;vertical-align:middle">
            <heading>News</heading>
          </td>
        </tr>
      </tbody></table>
      <!-- <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            
      </tbody></table> -->
      <!-- <ul>
        <li><strong>[March 2023]</strong> I will be joining Meta Reality Labs Research Pitt this summer for internship! </li>
        <li><strong>[March 2023]</strong> I will be joining Meta Reality Labs Research Pitt this summer for internship! </li>
      </ul> -->
      
      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
        <!-- <colgroup>
            <col width="15%">
            <col width="80%">
        </colgroup> -->
        <body>
	    <tr>
          <td valign="top" align="center"><strong>[4/2025]</strong></td>
          <td>I will co-organize a workshop about trustworthy FMs at ICCV 25:) </td>
          </tr>
	   <tr>
          <td valign="top" align="center"><strong>[5/2024]</strong></td>
          <td>I will work as a research intern at the deep learning group of Microsoft Research, Redmond. </td>
          </tr>
          <tr>
          <td valign="top" align="center"><strong>[1/2024]</strong></td>
          <td>I will work as an Erhu performer at the Traditional Chinese Ensemble Group of Rochester.</td>
          </tr>
      <tr>
        <td valign="top" align="center"><strong>[10/2021]</strong></td>
        <td>I will work as a research intern at the machine learning for sustainability group of Microsoft Research Asia, Beijing.
        </td>
      </tr>
        </tbody>
    </table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Research</heading>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
	<be>(* indicates the equal contribution with random author order. ‡ indicates the project leader.)<be>	


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/clip_unlearn.png" alt="decompose" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a><papertitle>Targeted Forgetting of Image Subgroups in CLIP Models
</papertitle></a>
        <br>
        <Strong>Zeliang Zhang</Strong>, Gaowen Liu, Charles Fleming, Ramana Rao Kompella, Chenliang Xu.
        <br>
        <em>CVPR</em>, 2025
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We propose a novel method to unlearn the CLIP on a subgroup of images.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>


 <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/vidcomposition.png" alt="decompose" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2411.10979"><papertitle>VidComposition: Can MLLMs Analyze Compositions in Compiled Videos?
</papertitle></a>
        <br>
        Yunlong Tang, Junjia Guo, Hang Hua, Susan Liang, Mingqian Feng, Xinyang Li, Rui Mao, Chao Huang, Jing Bi, <Strong>Zeliang Zhang</Strong>,  Pooyan Fazli, Chenliang Xu.
        <br>
        <em>CVPR</em>, 2025
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We propose a new benchmark specifically designed to evaluate the video composition understanding capabilities of MLLMs.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>




   <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/ICLR_attack.png" alt="decompose" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://openreview.net/pdf?id=ePJrZLIqpV"><papertitle>Rethinking Audio-Visual Adversarial Vulnerability from Temporal and Modality Perspectives
</papertitle></a>
        <br>
        <Strong>‡Zeliang Zhang*</Strong>, Susan Liang*, Daiki Shimada, Chenliang Xu.
        <br>
        <em>ICLR</em>, 2025
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We propose a powerful audio-visual adversarial attack and adversarial training defense method.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/flops.png" alt="decompose" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2410.05966"><papertitle>FLOPS: Forward Learning with OPtimal Sampling
</papertitle></a>
        <br>
        Tao Ren, Zishi Zhang, Jinyang Jiang, Guanghao Li, <Strong>Zeliang Zhang</Strong>, Mingqian Feng, Yijie Peng.
        <br>
        <em>ICLR</em>, 2025
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We propose to allocate the optimal number of queries during the forward-only training to balance estimation accuracy and computational efficiency.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/mllm_prune.png" alt="decompose" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2410.06169"><papertitle>Treat Visual Tokens as Text? But Your MLLM Only Needs Fewer Efforts to See</papertitle></a>
        <br>
        <Strong>Zeliang Zhang*</Strong>, Phu Pham*, ‡Wentian Zhao*, ‡Kun Wan*, Yu-Jhe Li, Daniel Miranda, Ajinkya Kale, Chenliang Xu.
        <br>
        <em>Preprint</em>, 2024
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We prune the visual-related computation in multiple MLLMs to accelerate the inference.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>
    
    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/decompose.png" alt="decompose" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2410.06851"><papertitle>Understanding Model Ensemble in Transferable Adversarial Attack</papertitle></a>
        <br>
        Wei Yao*, <Strong>Zeliang Zhang*</Strong>, Huayi Tang, Yong Liu.
        <br>
        <em>ICML</em>, 2025
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We provide early theoretical insights that serve as a roadmap for advancing model ensemble adversarial attack.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/clip_bias.png" alt="clipbias" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2409.15035"><papertitle>Can CLIP Count Stars? An Empirical Study on Quantity Bias in CLIP</papertitle></a>
        <br>
        <Strong>Zeliang Zhang</Strong>, Zhuo Liu, Mingqian Feng, Chenliang Xu.
        <br>
        <em>Findings of EMNLP</em>, 2024
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We empirically investigate the quantity bias in CLIP.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>



    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/hullu.png" alt="hullu" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2406.12663"><papertitle>Do More Details Always Introduce More Hallucinations in LVLM-basedImage Captioning?</papertitle></a>
        <br>
        Mingqian Feng, Yunlong Tang, <Strong>Zeliang Zhang</Strong>, Chenliang Xu.
        <br>
        <em>Preprint</em>, 2024
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>To alleviate the problem of hallucinations, we propose the Differentiated Beam Decoding (DBD), along with a reliable new set of evaluation metrics: CLIP-Precision, CLIP-Recall, and CLIP-F1.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>


    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/moe_pruning.png" alt="MoEPruning" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/pdf/2407.09590"><papertitle>Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts</papertitle></a>
        <br>
        <Strong>Zeliang Zhang</Strong>, Xiaodong Liu, Hao Cheng, Chenliang Xu, Jianfeng Gao.
        <br>
        <em>Preprint</em>, 2024
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>We propose a method of grouping and pruning similar experts to improve the model's parameter efficienc</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>

    <tr>
      <td style="padding:20px;width:25%;vertical-align:middle">
        <img src="images/vidLLM.png" alt="vidLLM" width="210" height="160">
      </td>
      <td width="75%" valign="middle">
          <a href="https://arxiv.org/abs/2312.17432"><papertitle>Video Understanding with Large Language Models: A Survey</papertitle></a>
        <br>
        Yunlong Tang, Jing Bi, Siting Xu, Luchuan Song, Susan Liang, Teng Wang, Daoan Zhang, Jie An, Jingyang Lin, Rongyi Zhu, Ali Vosoughi, Chao Huang, <Strong>Zeliang Zhang</Strong>, Feng Zheng, Jianguo Zhang, Ping Luo, Jiebo Luo, Chenliang Xu.
        <br>
        <em>Technical Report</em>, 2023
        <br>
        <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
        <p>This survey provides a detailed overview of the recent advancements in video understanding harnessing the power of LLMs.</p>
        <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
          <br>
          <em>CVPR Sight and Sound Workshop</em>, 2022 -->
          <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
      </td>
    </tr>


	        <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/pipeline.png" alt="pls_subclass" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Discover_and_Mitigate_Multiple_Biased_Subgroups_in_Image_Classifiers_CVPR_2024_paper.pdf"><papertitle>Discover Multiple Biased Subgroups in Image Classifiers</papertitle></a>
              <br>
              <strong>Zeliang Zhang*</strong>, Mingqian Feng*, Zhiheng Li, Chenliang Xu.
              <br>
              <em>CVPR</em>, 2024
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a novel method, namely DII (decomposition, identification, and interpretation), to debug the multi-bias of the models.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/l2t.png" alt="pls_subclass" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2405.14077"><papertitle>Learning to Transform Dynamically for Better Adversarial Transferability</papertitle></a>
              <br>
              Rongyi Zhu*, <strong>Zeliang Zhang*‡</strong>, Susan Liang, Zhuo Liu, Chenliang Xu.
              <br>
              <em>CVPR</em>, 2024
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a novel method, namely L2T (learn to transform), to boost the adversarial transferability.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>
		
    


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/gradient_estimation_CNN.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2305.08960.pdf"><papertitle>One Forward is Enough for Training Neural Networks via the Likelihood Ratio Method</papertitle></a>
              <br>
              Jinyang Jiang*, <strong>Zeliang Zhang*</strong>, Chenliang Xu, Zhaofei Yu, Yijie Peng.
              <br>
              <em>ICLR</em>, 2024
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We explore the potential of Likelihood ratio method for gradient estimation and train multi-architectures of NN without back-propagation.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tricks_paper.png" alt="point_denoise" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/abs/2401.08734"><papertitle>Bag of Tricks to Boost the Adversarial Transferability</papertitle></a>
              <br>
              <strong>Zeliang Zhang</strong>,  Wei Yao, Xiaosen Wang
              <br>
              <em>Technical  Report</em>, 2024
              <p>We propose a bag of novel tricks to boost the adversarial transferability among different models.</p>
            </td>
          </tr>




          


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/text_paper.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://aclanthology.org/2024.findings-eacl.83/"><papertitle>Random Smooth-based Certified Defense against Text Adversarial Attack</papertitle></a>
              <br>
              <strong>Zeliang Zhang*</strong>, Wei Yao*, Susan Liang, Chenliang Xu
              <br>
              <em>EACL Findings</em>, 2024
              <p> We propose to treat the word substitution as a continuous perturbation on the word embedding representation for better robustness. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/sit_paper.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2309.14700.pdf"><papertitle>Structure Invariant Transformation for better Adversarial Transferability</papertitle></a>
              <br>
              Xiaosen Wang, <strong>Zeliang Zhang</strong>, Jianping Zhang
              <br>
              <em>ICCV</em>, 2023
              <p> We propose a novel input transformation based attack, called Structure Invariant Transformation (SIA), which applies a random image transformation onto each image block to craft a set of diverse images for gradient calculation.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tensor_train.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://ieeexplore.ieee.org/abstract/document/10633902"><papertitle>High-performance Tensor-Train Primitives Using GPU Tensor Cores</papertitle></a>
              <br>
              Xiao-Yang Liu, Hao Hong, <strong>Zeliang Zhang</strong>, Weiqing Tong, Xiaodong Wang, Anwar Walid 
              <br>
              <em>IEEE Transaction on Computers</em>, 2024
              <p> We present high-performance tensor-train primitives using GPU tensor cores and demonstrate three applications.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/dhf_paper.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2304.10136.pdf"><papertitle>Diversifying the High-level Features for better Adversarial Transferability</papertitle></a>
              <br>
              Zhiyuan Wang*, <strong>Zeliang Zhang*</strong>, Siyuan Liang, Xiaosen Wang
              <br>
              <em>BMVC</em>, 2023, oral
              <p> We propose diversifying the high-level features (DHF) for more transferable adversarial examples.  </p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/fair.png" alt="extreme_compression" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
              <a href="https://arxiv.org/pdf/2207.04581.pdf"><papertitle>How Robust is your Fair Model? Exploring the Robustness of Diverse Fairness Strategies</papertitle></a>
              <br>
              Edward Small, Wei Shao, <strong>Zeliang Zhang</strong>, Peihan Liu, Jeffrey Chan, Kacper Sokol, Flora Salim
              <br>
              <em>Data Mining and Knowledge Discovery</em>, 2024
              <p> We quantitatively evaluate the robustness of fairness optimization strategies.  </p>
            </td>
          </tr>

	<tr >
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/quantum_paper.png" alt="av_nerf" width="210" height="120">
            </td>
            <td width="75%" valign="middle">
		<a href="https://openreview.net/pdf?id=T5ArxPU3Oq"><papertitle>Classical Simulation of Quantum Circuits: Parallel Environments and Benchmark</papertitle></a>
              <br>
              Xiao-Yang Liu, <strong>Zeliang Zhang</strong>
              <br>
              <em>NeurIPS dataset and benchmark track</em>, 2023
              <p>We develop a dozen of massively parallel environments to simulate quantum circuits. We open-source our parallel gym environments and benchmarks.</p>
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/tensor_paper.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://ieeexplore.ieee.org/abstract/document/9987675"><papertitle>High-Performance Tensor Learning Primitives Using GPU Tensor Cores</papertitle></a>
              <br>
              Xiao-Yang Liu*,  <strong>Zeliang Zhang*</strong>, Zhiyuan Wang, Han Lu, Xiaodong Wang, Anwar Walid.
              <br>
              <em>IEEE Transaction on Computers</em>, 2022
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose novel hardware-oriented optimization strategies for tensor learning primitives on GPU tensor cores.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/ta_paper.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2112.06569.pdf"><papertitle>Triangle Attack: A Query-efficient Decision-based Adversarial Attack</papertitle></a>
              <br>
              Xiaosen Wang, <strong>Zeliang Zhang</strong>, Kangheng Tong, Dihong Gong, Kun He, Zhifeng Li, Wei Liu
              <br>
              <em>ECCV</em>, 2022
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a novel Triangle Attack (TA) to optimize the perturbation by utilizing the geometric information that the longer side is always opposite the larger angle in any triangle.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle">
              <img src="images/case_paper.png" alt="lr_cnn" width="210" height="160">
            </td>
            <td width="75%" valign="middle">
                <a href="https://arxiv.org/pdf/2102.04450.pdf"><papertitle>Noise Optimization for Artificial Neural Networks</papertitle></a>
              <br>
              Li Xiao, <strong>Zeliang Zhang</strong>, Yijie Peng
              <br>
              Short paper: <em>CASE</em>, 2022; Long paper: <em>T-ASE</em>, 2024
              <br>
              <!-- <a href="project/ego-av-loc/index.html">project page</a> -->
              <p>We propose a new technique to compute the pathwise stochastic gradient estimate with respect to the standard deviation of the Gaussian noise added to each neuron of the ANN.</p>
              <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf"><papertitle>Audio-Visual Object Localization in Egocentric Videos</papertitle></a>
                <br>
                <em>CVPR Sight and Sound Workshop</em>, 2022 -->
                <!-- <a href="https://sightsound.org/papers/2022/Huang_Audio-Visual_Object_Localization_in_Egocentric_Videos.pdf">Paper</a> -->
            </td>
          </tr>

        </tbody></table>

				
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Education</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td tyle="padding:20px;width:25%;vertical-align:middle"><img src="images/uofr-logo-shield.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>University of Rochester </strong>, NY, USA
              <br>
              Ph.D. in Computer Science
              <br>
              Sep. 2022 - Present
              <br> 
              Advisor: <a href="https://www.cs.rochester.edu/~cxu22/p/index.html">Chenliang Xu</a>
            </td>
          </tr>
          <tr>
            <td tyle="padding:20px;width:25%;vertical-align:middle"><img src="images/hust.jpg"  width="120"></td>
            <td width="75%" valign="center">
              <strong>Huazhong University of Science and Technology</strong>, Wuhan, China
              <br>
              B.Eng in Computer Science and Technology
              <br>
              Sept. 2018 - Jun. 2022
            </td>
          </tr>
        </tbody></table>
        
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Experience</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					
          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/msr.jpg" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Microsoft Research </strong>, Redmond, US
              <br>
              Research intern, then part-time researcher
              <br>
              May 2024 - Nov 2024
              <br> 
              Advisor: Xiaodong Liu and Hao Cheng
              <br>
              Work on efficient training and inference of language models. 
            </td>
          </tr>


          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/msra.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Microsoft Research Asia </strong>, Beijing, China
              <br>
              Research intern
              <br>
              Oct. 2021 - Jun. 2022
              <br> 
              Advisor: Xinran Wei
              <br>
              Work on high-performance computation of DFT, which is important/bottleneck in material design using AI. 
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/cmb.png" width="120" ></td>
            <td width="75%" valign="center">
              <strong>Columbia University </strong>, NYC, US
              <br>
              Research assistant
              <br>
              Feb. 2020 - Dec. 2022
              <br> 
              Advisor: Xiao-Yang Liu
              <br>
              Work on high-performance tensor computation using GPUs and publish two workshop papers on top conference, namely the <a href=https://tensorworkshop.github.io/2020/accepted_papers/4-Trillion-Tensor-Trillion-Scale%20CP%20Tensor%20Decomposition.pdf>"Trillion-Tensor: Trillion-Scale CP Tensor Decomposition"</a> at IJCAI 2020 TNRML workshop and <a href=https://tensorworkshop.github.io/NeurIPS2020/accepted_papers/NIPS_2020_Workshop_Zeliang__Copy_%20(6).pdf>"Parallel TTr1-Tensor: Randomized Compression-based Scheme for Tensor Train Rank-1 Decomposition"</a> at NIPS 2020 QTNML workshop.
            </td>
          </tr>

        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Projects</heading>
            </td>
          </tr>
        </tbody></table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
					

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/erl.jpg" width="120" ></td>
            <td width="75%" valign="center">
              <a href="https://github.com/AI4Finance-Foundation/ElegantRL"><papertitle>Elegant RL </papertitle></a> (~ 3k stars! &#128640)
              Develop the RL-Hamiltonian algorithm to stablize the RL training and publish it as a <a href="https://tensorworkshop.github.io/NeurIPS2021/accepted_papers/DHN_Variational_RL_27.pdf">poster</a> in NIPS 2021 QTNML workshop.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/ta_overview.png" width="120" ></td>
            <td width="75%" valign="center">
              <a href="https://github.com/Trustworthy-AI-Group/TransferAttack"><papertitle>TransferAttack </papertitle></a> 
              One of the main contributors. TransferAttack is a pytorch framework to boost the adversarial transferability for image classification.
            </td>
          </tr>

          <tr>
            <td style="padding:20px;width:25%;vertical-align:middle"><img src="images/quantum_circuit.png" width="120" ></td>
            <td width="75%" valign="center">
              <a href="https://github.com/YangletLiu/RL4QuantumCircuits"><papertitle>RL for Quantum Circuits </papertitle></a> 
              One of the main contributors. Open-source benchmark and environments for the classical simulation of quantum circuits.  
            </td>
          </tr>

        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                The template is based on <a href="http://jonbarron.info/">Jon Barron</a>'s website.
              </p>
              <a href="https://clustrmaps.com/site/1bylr"  title="Visit tracker"><img src="//www.clustrmaps.com/map_v2.png?d=YawXL8l1D95PjTx_JBYthcSsuZ69aEDVBedBB4t0Gzs&cl=ffffff" /></a>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
